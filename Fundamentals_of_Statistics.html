<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Fundamentals_of_Statistics.utf8</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="syllabus.html">Course Syllabus</a>
</li>
<li>
  <a href="Fundamentals_of_Statistics.html">Fundamentals</a>
</li>
<li>
  <a href="Linear_Models.html">Linear Models</a>
</li>
<li>
  <a href="Probability.html">Probability</a>
</li>
<li>
  <a href="Maximum_Likelihood.html">Maximum Liklihood</a>
</li>
<li>
  <a href="AIC.html">AIC</a>
</li>
<li>
  <a href="Comp_Intensive_Approaches.html">Resampling Methods</a>
</li>
<li>
  <a href="study_notes.html">Study Notes</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<div id="introduction-to-statistical-analysis" class="section level1">
<h1><span class="header-section-number">1</span> Introduction to statistical analysis</h1>
<p>Kachigan (1986) defines statistical analysis as the:</p>
<ul>
<li><p>Collection,</p></li>
<li><p>Organization, and</p></li>
<li><p>Interpretation of data according to well-defined procedures</p></li>
</ul>
<p>In this course we will use Kachigan’s definition as a framework and focus on aspects of collection (experimental design), organization (using descriptive and inferential approaches), and interpretation.</p>
<p>One of the primary facets of quantitative analysis is the need to be creative in your approaches - the methods outlined in this class provide the foundation for analysis but the practitioner is encouraged to explore the methods and practices in their discipline to best address the needs for interpretation.</p>
<!-- ## The Research Process -->
<!-- A.J. Underwood described the "components of design in ecological field experiments" (2009) and this stereotyped approach can be used in the natural sciences and other disciplines. -->
<!-- Scientific investigation is most efficient and leads to strong inference when the  -->
<!-- ## Generating and Testing Theories -->
<!-- Field et al. (2012) -->
<!-- Theory  -->
<!-- + A hypothesized general principle or set of principles that explains known findings about a topic and from which new hypotheses can be generated. -->
<!-- Hypothesis -->
<!-- + A prediction from a theory.  -->
<!-- + E.g. the number of people turning up for a Big  Brother audition that have narcissistic personality disorder will be higher than the general level (1%) in the population. -->
<!-- Falsification  -->
<!-- + The act of disproving a theory or hypothesis. -->
<div id="data-collection" class="section level2">
<h2><span class="header-section-number">1.1</span> Data Collection</h2>
<p>Krebs: “Not everything that can be measured should be measured.”</p>
<p>Need to define variables.</p>
<p>Variables are anything that can be measured and exhibit variation</p>
<ul>
<li><p>among entities</p></li>
<li><p>among space</p></li>
<li><p>in time</p></li>
</ul>
<!-- ### The Research Process -->
</div>
<div id="types-of-measurement" class="section level2">
<h2><span class="header-section-number">1.2</span> Types of Measurement</h2>
<div id="categorical-entities-are-divided-into-distinct-categories" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Categorical (entities are divided into distinct categories):</h3>
<p>Binary variable: There are only two categories.</p>
<ul>
<li>Dead or alive.</li>
</ul>
<p>Nominal variable: There are more than two categories.</p>
<ul>
<li>Whether someone is an omnivore, vegetarian, vegan, or fruitarian.</li>
</ul>
<p>Ordinal variable: The same as a nominal variable but the categories have a logical order.</p>
<ul>
<li>Whether people got a fail, a pass, a merit or a distinction in their exam.</li>
</ul>
</div>
<div id="continuous-entities-get-a-distinct-score" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Continuous (entities get a distinct score):</h3>
<p>Interval variable: Equal intervals on the variable represent equal differences in the property being measured.</p>
<ul>
<li>e.g. the difference between 6 and 8 is equivalent to the difference between 13 and 15.</li>
</ul>
<!-- ### Data Collection 2: How to Measure -->
<!-- #### Correlational research (measurative): -->
<!-- Observing what naturally goes on in the world without directly interfering with it (Hurlburt). -->
<!-- #### Experimental research (manipulative): -->
<!-- One or more variable is systematically manipulated to see their effect (alone or in combination) on an outcome variable. -->
<!-- Statements can be made about cause and effect (Hurlburt). -->
<!-- ### Experimental Research Methods  -->
<!-- Cause and Effect (Hume, 1748) -->
<!-- + Cause and effect must occur close together in time (contiguity).  -->
<!-- + The cause must occur before an effect does. -->
<!-- + The effect should never occur without the presence of the cause. -->
</div>
</div>
<div id="data-managment-from-malin-pinsky" class="section level2">
<h2><span class="header-section-number">1.3</span> Data Managment (from Malin Pinsky)</h2>
<ol style="list-style-type: decimal">
<li><p>Keep lab notebooks to record what we did, learned, or produced each day. Can be physical notebooks, text files, Evernote, Jupyter notebooks, etc.</p></li>
<li><p>Establish a mechanism to facilitate collaboration and sharing within the lab.</p></li>
<li><p>Use descriptive file names</p></li>
<li><p>Keep our raw data in related to the project, unless the data files are too large.</p></li>
<li><p>Use a folder called “data” within the repository.</p></li>
<li><p>Store raw data with metadata describing what’s in the file and what the columns mean - what is metadata?</p></li>
<li><p>If we clean the data, we often use a folder called something like “data-raw” and a folder called “data-clean” to differentiate data in its original form from data that has been manipulated. Have “master” or “original” and “tidy” versions of files and name them appropriately.</p></li>
<li><p>If using data downloaded from another data source, we often have a folder called “data_dl” for downloaded data. Include the data source in a README file for reproducibility.</p></li>
<li><p>Work in the cloud (Dropbox, OneDrive)</p></li>
<li><p>We use scripts to process data, make models, do analyses, etc.</p></li>
</ol>
</div>
<div id="data-organization-broman-and-woo-2018" class="section level2">
<h2><span class="header-section-number">1.4</span> Data Organization (Broman and Woo, 2018)</h2>
<ul>
<li>Spreadsheets continue to to be a primary way for data storage, analysis, and visualization</li>
<li>Multipurpose (positive and negative)</li>
<li>Can be error prone to make large sweeping changes - hard to retrace your steps</li>
<li>Organization is critical for reproducible research and archiving</li>
</ul>
</div>
<div id="consistency" class="section level2">
<h2><span class="header-section-number">1.5</span> Consistency</h2>
<ol style="list-style-type: decimal">
<li><p>Use consistent codes for categorical variables</p></li>
<li><p>Use a consistent fixed code for any missing values</p></li>
<li><p>Use consistent variable names</p></li>
<li><p>Use a consistent data layout in multiple files</p></li>
<li><p>Use a consistent format for all dates</p></li>
<li><p>Use consistent phrases in your notes</p></li>
<li><p>Be careful about extra spaces within cells</p></li>
</ol>
</div>
<div id="variable-naming-convention" class="section level2">
<h2><span class="header-section-number">1.6</span> Variable naming convention</h2>
<ul>
<li><p>As a general rule, do not use spaces, either in variable names or file names</p></li>
<li><p>Instead, use underscores or perhaps hyphens</p></li>
</ul>
<p>Also, why these:</p>
<ul>
<li>no empty cells - hmm, maybe</li>
<li>one item per cell</li>
<li>recatangular data layout</li>
<li>don’t use font and cell colors as annotation</li>
<li>.csv backups</li>
</ul>
<p><Br></p>
</div>
</div>
<div id="univariate-frequency-distributions" class="section level1">
<h1><span class="header-section-number">2</span> Univariate Frequency Distributions</h1>
<p>-Relative Frequency</p>
<p>-Cumulative frequency</p>
<ul>
<li><p>A tally of how frequently occurring a value is among a set of measured objects.</p></li>
<li><p>What does a qualitative evaluation of the frequency distribution allow?</p></li>
<li><p>Data reduction technique with a tradeoff</p></li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<center>
<p>Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data</p>
<table>
<thead>
<tr class="header">
<th>Phosphorous concentration</th>
<th>Frequency</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>8.1 to 8.2</td>
<td>2</td>
</tr>
<tr class="even">
<td>8.2 to 8.3</td>
<td>6</td>
</tr>
<tr class="odd">
<td>8.3 to 8.4</td>
<td>8</td>
</tr>
<tr class="even">
<td>8.4 to 8.5</td>
<td>11</td>
</tr>
<tr class="odd">
<td>8.5 to 8.6</td>
<td>17</td>
</tr>
<tr class="even">
<td>8.6 to 8.7</td>
<td>17</td>
</tr>
<tr class="odd">
<td>8.7 to 8.8</td>
<td>24</td>
</tr>
<tr class="even">
<td>8.8 to 8.9</td>
<td>18</td>
</tr>
<tr class="odd">
<td>8.9 to 9.0</td>
<td>13</td>
</tr>
<tr class="even">
<td>9.0 to 9.1</td>
<td>10</td>
</tr>
<tr class="odd">
<td>9.1 to 9.2</td>
<td>4</td>
</tr>
</tbody>
</table>
Total frequency = 130 = n
</center>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<div id="cumulative-frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.1</span> Cumulative Frequency Distributions</h2>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data</p>
<table>
<colgroup>
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>Cummulative Frequency</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Phosphorous concentration</td>
<td>Frequency</td>
<td>Starting with Low Values</td>
<td>Starting with High Values</td>
</tr>
<tr class="even">
<td>8.15 to 8.25</td>
<td>2</td>
<td>2</td>
<td>130</td>
</tr>
<tr class="odd">
<td>8.25 to 8.35</td>
<td>6</td>
<td>8</td>
<td>128</td>
</tr>
<tr class="even">
<td>8.35 to 8.45</td>
<td>8</td>
<td>16</td>
<td>122</td>
</tr>
<tr class="odd">
<td>8.45 to 8.55</td>
<td>11</td>
<td>27</td>
<td>114</td>
</tr>
<tr class="even">
<td>8.55 to 8.65</td>
<td>17</td>
<td>44</td>
<td>130</td>
</tr>
<tr class="odd">
<td>8.65 to 8.75</td>
<td>17</td>
<td>61</td>
<td>86</td>
</tr>
<tr class="even">
<td>8.75 to 8.85</td>
<td>24</td>
<td>85</td>
<td>69</td>
</tr>
<tr class="odd">
<td>8.85 to 8.95</td>
<td>18</td>
<td>103</td>
<td>45</td>
</tr>
<tr class="even">
<td>8.95 to 9.05</td>
<td>13</td>
<td>116</td>
<td>27</td>
</tr>
<tr class="odd">
<td>9.05 to 9.15</td>
<td>10</td>
<td>126</td>
<td>14</td>
</tr>
<tr class="even">
<td>9.15 to 9.25</td>
<td>4</td>
<td>130</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Total frequency = 130 = n</p>
<div id="what-interval-to-choose" class="section level3">
<h3><span class="header-section-number">2.1.1</span> What Interval to Choose?</h3>
<p>Domain knowledge</p>
</div>
</div>
<div id="frequency-distributions" class="section level2">
<h2><span class="header-section-number">2.2</span> Frequency Distributions</h2>
<div id="empirical-and-theoretical" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Empirical and Theoretical</h3>
<p>Empirical: observed finite</p>
<p>Theoretical: infinite number of observations</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="the-normal-distribution" class="section level1">
<h1><span class="header-section-number">3</span> The Normal Distribution</h1>
<ul>
<li>The equation for the normal (Gaussian) distribution:</li>
</ul>
<center>
<span class="math inline">\(Y_i = \frac{1}{\sigma\sqrt2\pi}e^{-(X_i-\mu)^2/2\sigma^2}\)</span>
</center>
<ul>
<li>Lets looks at some of the general characteristics of this model</li>
<li>What are the parameters</li>
<li>What are characteristics of parameters</li>
<li>How do the parameters control/determine the shape</li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p><br></p>
<ul>
<li>Different Means, Identical Standard Deviation</li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><br></p>
<ul>
<li>Same Mean, Different Standard Deviation</li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div id="sample-from-a-normal-distribution" class="section level2">
<h2><span class="header-section-number">3.1</span> Sample from a Normal Distribution</h2>
<p>Normal distribution sampling theorem:</p>
<ul>
<li>Sampling distribution is normal when the population distribution is normal.</li>
<li>Sample mean = population mean</li>
<li>Sample sd = population <em>s</em></li>
</ul>
<div id="central-limit-theorem" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Central limit theorem</h3>
<ul>
<li>The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size becomes larger regardless of the population distribution shape.</li>
</ul>
</div>
</div>
<div id="parameter-estimation" class="section level2">
<h2><span class="header-section-number">3.2</span> Parameter Estimation</h2>
<p>Goal:</p>
<ul>
<li>Achieve and unbiased estimate - Long-run (infinite sampling)</li>
<li>Derive an efficient estimator - Fewest number samples to obtain accurate value</li>
</ul>
</div>
<div id="the-normal-distribution-your-first-statistical-model" class="section level2">
<h2><span class="header-section-number">3.3</span> The normal distribution, your first statistical model</h2>
<p><span class="math inline">\(outcome_i = (model) + error_i\)</span></p>
<p><span class="math inline">\(Y_{i,obs} = \frac{1}{\sigma\sqrt2\pi}e^{-(X_i-\mu)^2/2\sigma^2} + error_{i}\)</span></p>
<ul>
<li><p>In statistics we fit models to our data (i.e. we use a statistical model to represent what is happening in the world)</p></li>
<li><p>The mean, the measure of central tendency</p></li>
<li><p>The mean is the hypothetical value of the most common outcome. However, it is not a perfect representation of the characteristics of the data.</p></li>
<li><p>The Standard Deviation is the measure of dispersion (precision)</p></li>
<li><p>How can we assess how well the mean represents reality?</p></li>
</ul>
</div>
<div id="quantifying-error" class="section level2">
<h2><span class="header-section-number">3.4</span> Quantifying Error</h2>
<ul>
<li><p>A deviation is the difference between the mean (<em>expected</em>) and the <em>observed</em> data (the outcome of the sample).</p></li>
<li><p>The deviation of <em>observed</em> and <em>expected</em> value is also called: the residual, error, or residual error</p></li>
<li><p>Deviations can be calculated by taking each score and subtracting the mean from it:</p></li>
</ul>
<p>When the normal fitting models:</p>
<center>
<span class="math inline">\(deviation = x_i - \bar{x}\)</span>
</center>
<center>
<span class="math inline">\(error_i = outcome_i - (model_i)\)</span>
</center>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>Should we use the Total Error as an estimate of uncertainty?</p>
<ul>
<li>We could sum <span class="math inline">\(i^{th}\)</span> error terms from 1 to n.</li>
</ul>
<table>
<thead>
<tr class="header">
<th>Score</th>
<th>Mean</th>
<th>Deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.6</td>
<td>-1.6</td>
</tr>
<tr class="even">
<td>2</td>
<td></td>
<td>-0.6</td>
</tr>
<tr class="odd">
<td>3</td>
<td></td>
<td>0.4</td>
</tr>
<tr class="even">
<td>3</td>
<td></td>
<td>0.4</td>
</tr>
<tr class="odd">
<td>4</td>
<td></td>
<td>1.4</td>
</tr>
<tr class="even">
<td></td>
<td>Sum =</td>
<td>0</td>
</tr>
</tbody>
</table>
<center>
<span class="math inline">\(\Sigma(x_i - \bar{x}) = 0\)</span>
</center>
<div id="sum-of-squared-errors" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Sum of Squared Errors</h3>
<ul>
<li><p>We could add the deviations to find out the total error, but the deviations ‘cancel out’ (some are positive and others negative)</p></li>
<li><p>Therefore, we square each deviation.</p></li>
<li><p>If we add these squared deviations we get the sum of squared errors (SS).</p></li>
</ul>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Score</th>
<th>Mean</th>
<th>Deviation</th>
<th>Sqaured Deviation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>2.6</td>
<td>-1.6</td>
<td>2.56</td>
</tr>
<tr class="even">
<td>2</td>
<td></td>
<td>-0.6</td>
<td>0.36</td>
</tr>
<tr class="odd">
<td>3</td>
<td></td>
<td>0.4</td>
<td>0.16</td>
</tr>
<tr class="even">
<td>3</td>
<td></td>
<td>0.4</td>
<td>0.16</td>
</tr>
<tr class="odd">
<td>4</td>
<td></td>
<td>1.4</td>
<td>1.96</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>Total</td>
<td>5.20</td>
</tr>
</tbody>
</table>
<center>
<span class="math inline">\(SS = \Sigma(X - \bar{X})^2 = 5.20\)</span>
</center>
<ul>
<li>The sum of squares is a good measure of overall variability, but is dependent on the number of scores.</li>
</ul>
</div>
<div id="variance" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Variance</h3>
<ul>
<li><p>We calculate the average variability (average variability of each sample) by dividing by the number of scores minus one (n-1).</p></li>
<li><p>The quantity n - 1 is termed <em>degrees of freedom</em>. It is the maximum number of logically independent values, those with the freedom to vary in the sample</p></li>
<li><p>The sum of squares divided by the degrees of freedom is called the variance (s<sup>2</sup>).</p></li>
</ul>
<center>
<span class="math inline">\(variance (s^2) = \frac{SS}{n-1} = \frac{\Sigma(x_i-\bar{x})^2}{n-1} = \frac{5.20}{4} = 1.3\)</span>
</center>
</div>
<div id="standard-deviation" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Standard Deviation</h3>
<ul>
<li><p>The variance has one problem: it is measured in units<sup>2</sup>.</p></li>
<li><p>This isn’t a very meaningful metric so we take the square root value.</p></li>
<li><p>This is the standard deviation(s).</p></li>
</ul>
<p><span class="math inline">\(s = \sqrt\frac{\Sigma^n_{i=1}(x_i-\bar{x})^2}{n} = \sqrt\frac{5.20}{5} = 1.02\)</span></p>
<p><Br></p>
</div>
<div id="summary-of-variance-estimates" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Summary of variance estimates</h3>
<p>The sum of squares, variance, and standard deviation represent the same thing:</p>
<ul>
<li>The fit of the mean to the data, how well the mean represents the observed data</li>
<li>The variability in the data when modeled using the mean</li>
</ul>
<!-- \begin{aligned} -->
<!-- outcome_{lecturer1}&=(\bar{X}) + error_{lecturer1}\\ -->
<!-- &=2.6+error_{lecturer1} -->
<!-- \end{aligned} -->
</div>
</div>
<div id="other-meansures-of-variation" class="section level2">
<h2><span class="header-section-number">3.5</span> Other meansures of variation</h2>
<div id="range" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Range</h3>
<p>The Range</p>
<ul>
<li>The smallest score subtracted from the largest</li>
</ul>
<p>Example</p>
<ul>
<li>Number of friends of 11 Facebook users.</li>
<li>22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252</li>
<li>Range = 252 - 22 = 230</li>
<li>Very biased by outliers, why?</li>
</ul>
</div>
<div id="the-interquartile-range" class="section level3">
<h3><span class="header-section-number">3.5.2</span> The Interquartile Range</h3>
<ul>
<li>The values that split the sorted data into four equal parts.</li>
<li>First or lower quartile (the range values of the first 25% of values in ordered sequence)</li>
<li>Second quartile (the range values of the first 25 to 50% of values in ordered sequence)</li>
<li>Third quartile (the range values of the first 50 to 75% of values in ordered sequence)</li>
<li>Fourth quartile (the range values of the first 75 to 100% of values in ordered sequence)</li>
</ul>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p><em>How does the Interquartile Range vary between the two histograms?</em></p>
<p><Br></p>
</div>
</div>
<div id="z-scores" class="section level2">
<h2><span class="header-section-number">3.6</span> <em>Z</em>-scores</h2>
<div id="properties-of-z-scores---centering-and-scaling" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Properties of <em>Z</em>-scores - Centering and Scaling</h3>
<ul>
<li>Standardizing a score with respect to the other scores in the group.</li>
<li>Expresses a score in terms of how many standard deviations it is away from the mean.</li>
<li>Converts a distribution to a z-score distribution with mean of 0 and SD = 1.</li>
</ul>
<center>
<span class="math inline">\(z = \frac{X-\bar{X}}{S}\)</span>
</center>
<p><Br></p>
</div>
<div id="properties-of-z-scores---quantiles" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Properties of <em>Z</em>-scores - Quantiles</h3>
<ul>
<li><p>1.96 definesthe top 2.5% of the distribution.</p></li>
<li><p>-1.96 defines the bottom 2.5% of the distribution.</p></li>
<li><p>As such, 95% of z-scores lie between -1.96 and 1.96.</p></li>
<li><p>99% of z-scores lie between -2.58 and 2.58.</p></li>
<li><p>99.9% of them lie between -3.29 and 3.29.</p></li>
<li><p>Let’s look at a <a href="http://www.z-table.com/">Z Table</a></p></li>
</ul>
</div>
<div id="areas-under-the-normal-curve-for-different-quantile-values" class="section level3">
<h3><span class="header-section-number">3.6.3</span> Areas Under the Normal Curve for different quantile values</h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="probability-sampling-and-parameters" class="section level1">
<h1><span class="header-section-number">4</span> Probability, Sampling, and Parameters</h1>
<div id="probability-sampling-and-parameters-1" class="section level3">
<h3><span class="header-section-number">4.0.1</span> Probability, Sampling, and Parameters</h3>
<ul>
<li>Probability</li>
<li>Random variables <!-- + Sampling from a normal distribution --> <!-- + Sampling from a bionomial distribution --> <!-- + Parameter Estimation --> <!-- + Student's t-distribution --></li>
</ul>
</div>
<div id="probability" class="section level3">
<h3><span class="header-section-number">4.0.2</span> Probability</h3>
<p>SETS: A collection of items.</p>
<p>Element: on item of a set.</p>
<p>Subsets have multiple elements and are themselves sets.</p>
<p>Outcome set.</p>
<ul>
<li>In an experiment (or other phenomenon that yields results to observe), there is a set (usually very large) of possible outcomes. Let us refer to this as the outcome set.</li>
</ul>
<p>Intersect - the common elements in two sets.</p>
<p>Mutually exclusive - Sets with no elements in common , null intersect.</p>
<p>Union - the combination of elements in two sets - what element is in either set or both?</p>
<p>Complement - the remainder of outcomes in a set that are not in a subset</p>
</div>
<div id="sampling-distributions" class="section level3">
<h3><span class="header-section-number">4.0.3</span> Sampling Distributions</h3>
<p>Random samples:</p>
<ul>
<li>Every possible member of the population has an equal probability of being included in the sample.</li>
<li>Ex: scientific exit polling vs. twitter polls</li>
<li>Think of a normally distributed frequency distribution</li>
<li>Only statistically valid data that can be used for analysis - the first assumption of parametric statistics.</li>
</ul>
</div>
</div>
<div id="hypothesis-testing-and-power" class="section level1">
<h1><span class="header-section-number">5</span> Hypothesis Testing and Power</h1>
<p>Hypothesis Testing</p>
<p>Power</p>
<p>Assumptions of parametric statistics</p>
<div id="statistical-hypothesis-testing" class="section level3">
<h3><span class="header-section-number">5.0.1</span> Statistical Hypothesis Testing</h3>
<p>State:</p>
<ul>
<li>H<sub>O</sub></li>
<li>H<sub>A</sub></li>
</ul>
<p>Declare</p>
<ul>
<li>Alpha level</li>
</ul>
<p>Collect Data</p>
<p>Compare the test statistic to the critical value (determined by alpha)</p>
<p>State the resulting probability</p>
<p>State testable hypothesis</p>
<ul>
<li>These are a set of mutually exclusive and exhaustive outcomes.</li>
<li>Test statistic will support one or the other.</li>
<li>H<sub>O</sub> <sub>Null</sub><br />
</li>
<li>H<sub>A</sub> <sub>Alternative</sub></li>
</ul>
<p><Br></p>
<p><span class="math inline">\(H_0: \mu = 0, H_A:\mu \ne 0\)</span></p>
<p><span class="math inline">\(H_0: \mu = 3.5 cm, H_A:\mu \ne 3.5 cm\)</span></p>
<p><span class="math inline">\(H_0: \mu = 10.5 kg, H_A:\mu \ne 10.5 kg\)</span></p>
<p><Br></p>
</div>
<div id="example-use-z-score-to-test-mean" class="section level3">
<h3><span class="header-section-number">5.0.2</span> Example: Use <em>z</em>-score to Test Mean</h3>
<p>Is the mean fuel consumption of a population of busses equal to 20 mpg?</p>
<p>What is the null hypothesis?</p>
<p>We need information about the population</p>
<ul>
<li>Mean</li>
<li>Population standard deviation</li>
<li>Calculate z-score</li>
<li>What is the probability that the mean is 20 mpg given:
<ul>
<li>Sigma = 0.3, Mean = 19.1</li>
</ul></li>
</ul>
</div>
<div id="evaluate-z-score" class="section level3">
<h3><span class="header-section-number">5.0.3</span> Evaluate <em>z</em>-score</h3>
<p>What is the probability that we would get this <em>z</em>-score?</p>
<p>Hypothetical <em>z</em>-scores</p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p><em>Figure 4.3.1</em></p>
<p><Br> <Br></p>
<table>
<thead>
<tr class="header">
<th>z</th>
<th>.00</th>
<th>.01</th>
<th>.02</th>
<th>.03</th>
<th>.04</th>
<th>.05</th>
<th>.06</th>
<th>.07</th>
<th>.08</th>
<th>.09</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0.0</td>
<td>.5000</td>
<td>.5040</td>
<td>.5080</td>
<td>.5120</td>
<td>.5160</td>
<td>.5199</td>
<td>.5239</td>
<td>.5279</td>
<td>.5319</td>
<td>.5359</td>
</tr>
<tr class="even">
<td>0.1</td>
<td>.5398</td>
<td>.5438</td>
<td>.5478</td>
<td>.5517</td>
<td>.5557</td>
<td>.5596</td>
<td>.5636</td>
<td>.5675</td>
<td>.5714</td>
<td>.5753</td>
</tr>
<tr class="odd">
<td>0.2</td>
<td>.5793</td>
<td>.5832</td>
<td>.5871</td>
<td>.5910</td>
<td>.5948</td>
<td>.5987</td>
<td>.6026</td>
<td>.6064</td>
<td>.6103</td>
<td>.6141</td>
</tr>
<tr class="even">
<td>0.3</td>
<td>.6179</td>
<td>.6217</td>
<td>.6255</td>
<td>.6293</td>
<td>.6331</td>
<td>.6368</td>
<td>.6406</td>
<td>.6443</td>
<td>.6480</td>
<td>.6517</td>
</tr>
<tr class="odd">
<td>0.4</td>
<td>.6554</td>
<td>.6591</td>
<td>.6628</td>
<td>.6664</td>
<td>.6700</td>
<td>.6736</td>
<td>.6772</td>
<td>.6808</td>
<td>.6844</td>
<td>.6879</td>
</tr>
<tr class="even">
<td>0.5</td>
<td>.6915</td>
<td>.6950</td>
<td>.6985</td>
<td>.7019</td>
<td>.7054</td>
<td>.7088</td>
<td>.7123</td>
<td>.7157</td>
<td>.7190</td>
<td>.7224</td>
</tr>
<tr class="odd">
<td>0.6</td>
<td>.7257</td>
<td>.7291</td>
<td>.7324</td>
<td>.7357</td>
<td>.7389</td>
<td>.7422</td>
<td>.7454</td>
<td>.7486</td>
<td>.7517</td>
<td>.7549</td>
</tr>
<tr class="even">
<td>0.7</td>
<td>.7580</td>
<td>.7611</td>
<td>.7642</td>
<td>.7673</td>
<td>.7704</td>
<td>.7734</td>
<td>.7764</td>
<td>.7794</td>
<td>.7823</td>
<td>.7852</td>
</tr>
<tr class="odd">
<td>0.8</td>
<td>.7881</td>
<td>.7910</td>
<td>.7939</td>
<td>.7967</td>
<td>.7995</td>
<td>.8023</td>
<td>.8051</td>
<td>.8078</td>
<td>.8106</td>
<td>.8133</td>
</tr>
</tbody>
</table>
<p><em>Table 4.3.1:  Standard Normal Posibilities</em></p>
<p><Br></p>
</div>
<div id="is-it-meaningful-significance-level" class="section level3">
<h3><span class="header-section-number">5.0.4</span> Is it meaningful? Significance Level</h3>
<p>Declare</p>
<ul>
<li>Alpha level</li>
<li>p vs. alpha</li>
<li>Define prior to test</li>
<li>Two tail and one tail test</li>
</ul>
</div>
<div id="alpha" class="section level3">
<h3><span class="header-section-number">5.0.5</span> <em>alpha</em></h3>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p><em>Figure 4.5.1</em></p>
<p><Br></p>
</div>
<div id="statistical-hypothesis-testing-1" class="section level3">
<h3><span class="header-section-number">5.0.6</span> Statistical Hypothesis Testing</h3>
<p>Ex: Look to see if the population mean is not different from some specified value.</p>
<p>H<sub>O</sub>: u = 0</p>
<p>H<sub>A</sub>: u is not equal 0</p>
<p>Introduce the idea of a critical value</p>
<ul>
<li>Alpha level of 0.05</li>
</ul>
<p>We have data taken from the weight change in horses given some medical treatment.</p>
<p>We are interested to know if the mean change in weight that we found +1.29 kg is significantly different from 0 kg.</p>
<ul>
<li>We calculate the z-score and find that Z = 1.45</li>
</ul>
<p><span class="math inline">\(P(mean \ge 1.29) = P(Z \ge 1.45) = ?\)</span></p>
<p><span class="math inline">\(P(mean \le 1.29) = P(Z \le 1.45) = ?\)</span></p>
<p>Z = 1.96 is the rejection region at 2.5%</p>
<ul>
<li>This is the ‘region of rejection’</li>
</ul>
<p>Now we have a way to objectively reject or accept the null hypothesis.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><em>Figure 4.6.1</em></p>
<p><Br></p>
</div>
<div id="one--and-two-tailed-tests" class="section level3">
<h3><span class="header-section-number">5.0.7</span> One- and Two-Tailed Tests</h3>
<p>Alternative to testing ‘is the value different.’</p>
<p>In some cases we care about the direction of the difference.</p>
<p>Use one-tailed test</p>
<ul>
<li>In general, one-tailed hypotheses about a mean are:
<ul>
<li><span class="math inline">\(H_0:\mu\ge\mu_0\)</span> and <span class="math inline">\(H_A:\mu&lt;\mu_0\)</span></li>
</ul></li>
<li>In which case, H<sub>0</sub> is rejected if the test statistic is in the left-hand tail of the distribution or:
<ul>
<li><span class="math inline">\(H_0:\mu\le\mu_0\)</span> and <span class="math inline">\(H_A:\mu&gt;\mu_0\)</span></li>
</ul></li>
</ul>
<p>Contrast the region of rejection for these.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p><em>Figures 4.7.1 and 4.7.2</em></p>
<p><Br></p>
</div>
<div id="type-1-and-type-2-errors" class="section level3">
<h3><span class="header-section-number">5.0.8</span> Type-1 and Type-2 Errors</h3>
<p>Sometimes we:</p>
<ul>
<li>Reject the null hypothesis when it is true.</li>
<li>Accept the alternative hypothesis when it is false.</li>
</ul>
<p>Type 1 error or alpha error - frequency of rejecting H<sub>0</sub> when it is true.</p>
<p>Type 1 error rate is equal to alpha.</p>
<p>Type 1 error: “rejecting the null hypothesis when it is true.”</p>
<p>Type 1 error or ‘<span class="math inline">\(\alpha\)</span> error’ is equal to <span class="math inline">\(\alpha\)</span></p>
<p>Now we have some criteria to choose alpha.</p>
<p>So if your <span class="math inline">\(\alpha\)</span>, or critical value is 0.10</p>
<ul>
<li>We have a 10% probability of rejecting the null hypothesis when we should have, in fact, accepted it.</li>
</ul>
<div id="type-1-alpha-error" class="section level4">
<h4><span class="header-section-number">5.0.8.1</span> Type 1 (<span class="math inline">\(\alpha\)</span>) Error</h4>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p><em>Figure 4.8.1</em></p>
<p><Br></p>
<p>Type 2 error: “accepting the null hypothesis when it is false.”</p>
<p>Type 2 error or ‘<span class="math inline">\(\beta\)</span> error’ is equal to <span class="math inline">\(\beta\)</span>.</p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>If H<sub>0</sub> is true</th>
<th>If H<sub>0</sub> is false</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>If H<sub>0</sub> is rejected</td>
<td>Type I error</td>
<td>No error</td>
</tr>
<tr class="even">
<td>If H<sub>0</sub> is not rejected</td>
<td>No error</td>
<td>Type II error</td>
</tr>
</tbody>
</table>
<p><em>Table 4.8.1:  Two Types of Errors in Hypothesis Testing</em></p>
<p><Br></p>
<p>Thought experiments:</p>
<ul>
<li>Ex. Endangered species conservation</li>
<li>Ex. Pharmaceutical testing</li>
</ul>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th></th>
<th>If H<sub>0</sub> is true</th>
<th>If H<sub>0</sub> is false</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>If H<sub>0</sub> is rejected</td>
<td><span class="math inline">\(\alpha\)</span></td>
<td><span class="math inline">\(1-\beta\)</span> (“power”) No error</td>
</tr>
<tr class="even">
<td>If H<sub>0</sub> is not rejected</td>
<td>No error <span class="math inline">\(1-\alpha\)</span></td>
<td><span class="math inline">\(\beta\)</span></td>
</tr>
</tbody>
</table>
<p><em>Table 4.8.2:  Long-term Probabilities of Outcomes in Hypothesis Testing</em></p>
<p><Br></p>
</div>
</div>
<div id="power" class="section level3">
<h3><span class="header-section-number">5.0.9</span> Power</h3>
<p>Power: the probability that a statistical test will reject a null hypothesis when it is false (proper rejection).</p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p><em>Figure 4.9.1</em></p>
<p><Br> <Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p><em>Figure 4.9.2</em></p>
<p><Br></p>
<div id="leafs-power-simulation-in-r" class="section level4">
<h4><span class="header-section-number">5.0.9.1</span> Leaf’s power simulation in R</h4>
<p><Br></p>
</div>
</div>
<div id="what-influences-statistical-power" class="section level3">
<h3><span class="header-section-number">5.0.10</span> What Influences Statistical Power?</h3>
</div>
<div id="assumptions" class="section level3">
<h3><span class="header-section-number">5.0.11</span> Assumptions</h3>
<p>Assumptions - When broken then we are not able to make inference or accurate descriptions about reality.</p>
<p>Thus our models are flawed descriptions and inferences will be compromised.</p>
<ul>
<li>Assumptions of parametric tests based on the normal distribution.</li>
<li>Understand the assumption of normality.</li>
<li>Understand homogeneity of variance.</li>
<li>Know how to correct problems (with respect to the assumptions of normality) in the data.</li>
</ul>
<p>Parametric tests based on the normal distribution assume:</p>
<ul>
<li>Normally distributed
<ul>
<li>Distribution of samples</li>
<li>Model distribution (residuals)</li>
</ul></li>
<li>Homogeneity of variance</li>
<li>Interval or ratio level data
<ul>
<li>Some data are intrinsically not normally distributed.</li>
</ul></li>
<li>Independence of observation</li>
</ul>
</div>
<div id="the-normal-distribution-review" class="section level3">
<h3><span class="header-section-number">5.0.12</span> 4.12 The Normal Distribution Review</h3>
<p>Commonly the distribution of measurements (frequency of data collected from interval data) have a bell shaped distribution</p>
<p>Parameters of the model determine its shape.</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p><em>Figure 4.12.1</em></p>
<p><Br></p>
<p>Two-parameter distribution</p>
<p><span class="math inline">\(f(x,\mu,\sigma)=\frac{1}{\sigma\sqrt2\pi}e^-(x-\mu)^2/2\sigma^2\)</span></p>
<p>Symmetric around mu</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p><em>Figure 4.12.2</em></p>
<p><Br></p>
</div>
<div id="distribution-of-samples-central-limit-theorem" class="section level3">
<h3><span class="header-section-number">5.0.13</span> 4.13 Distribution of Samples Central Limit Theorem</h3>
<p>How well do samples represent the population?</p>
<p>A key foundation of frequentist statistics - samples are random variables: When we take a sample from the population we are taking one of many possible samples.</p>
<p>Thought experiment - take many, many samples from a population.</p>
<p>Do we expect all sample to have the same mean value (the same sample mean)?</p>
<ul>
<li>No, there is variation in the samples - ‘Sampling variation.’</li>
</ul>
<p>The frequency histogram of samples is the sampling distribution.</p>
<p>Analog to standard deviation</p>
<ul>
<li>SD how well does model fit the data</li>
</ul>
<p>We can take the standard deviation of the sample mean.</p>
<ul>
<li>Termed ‘Standard Error of the Sampling mean’</li>
<li>Or ‘Standard Error’</li>
</ul>
<p>If the sample is large then sampling error can be approximated:</p>
<center>
<span class="math inline">\(SE=\frac{s}{\sqrt n}\)</span>
</center>
<p><Br></p>
</div>
<div id="assumption-1-sample-observations-and-associated-deviations-are-normally-distributed." class="section level3">
<h3><span class="header-section-number">5.0.14</span> 4.14 Assumption #1: Sample observations and associated deviations are normally distributed.</h3>
<p>We will review how to check these assumptions in this lecture and the following lab.</p>
</div>
<div id="assumption-2-homogeneity-of-variance." class="section level3">
<h3><span class="header-section-number">5.0.15</span> 4.15 Assumption #2: Homogeneity of Variance.</h3>
<p>Data taken from groups must have homogenous variance.</p>
<p>Homogenous does not mean ‘equal’ but equal in the probabilistic sense.</p>
<p>We will review how to check these assumptions in this lecture and the following lab.</p>
</div>
<div id="assumption-3-interval-and-ratio-scale" class="section level3">
<h3><span class="header-section-number">5.0.16</span> 4.16 Assumption #3: Interval and Ratio Scale</h3>
<p>Continuous variables</p>
<ul>
<li>Interval scale (equal intervals between measurements).</li>
<li>Ratio scale - Conversion of interval data such that ratio of measurements was meaningful.</li>
</ul>
<p>Ordinal data - rankings -</p>
<ul>
<li>Darker, faster, shorter and might label these 1,2,3,4,5 to reflect increases in magnitude.</li>
<li>Really convey less information - data condensation.</li>
</ul>
<p>Nominal or Categorical data</p>
<ul>
<li>Example are public surveys</li>
<li>Willingness to vote for a candidate? Economic class, Taxonomic categories.</li>
</ul>
</div>
<div id="assumption-4" class="section level3">
<h3><span class="header-section-number">5.0.17</span> 4.17 Assumption #4</h3>
<p>Observations are independent</p>
<p>The measurement of one sample does not influence the measurement of another sample.</p>
<ul>
<li>Measurements taken in space and time are examples - experimenter needs to determine when there is zero correlation between the samples.</li>
<li>Behavioral Example - the opinion of one person influences the behavior of another person and hence the measurements are correlated.</li>
</ul>
</div>
<div id="assessing-normality" class="section level3">
<h3><span class="header-section-number">5.0.18</span> 4.18 Assessing Normality</h3>
<p>We don’t have access to the population distribution so we usually test the observed data</p>
<p>Graphical displays</p>
<ul>
<li>Q-Q plot (or P-P plot)</li>
<li>Histogram</li>
</ul>
<p>Kolmogorov-Smirnov</p>
<p>Shapiro-Wilk</p>
</div>
<div id="assessing-homogeneity-of-variance" class="section level3">
<h3><span class="header-section-number">5.0.19</span> 4.19 Assessing Homogeneity of Variance</h3>
<p>Figures</p>
<p>Levene’s test</p>
<ul>
<li>Tests if variances in different groups are the same.</li>
<li>Significant = variances not ‘equal’</li>
<li>Non-significant = variances are ‘equal’</li>
</ul>
<p>Variance ratio</p>
<ul>
<li>With 2 or more groups</li>
<li>VR = largest variance/smallest variance</li>
<li>If VR &lt; 2, homogeneity can be assumed</li>
</ul>
</div>
<div id="correcting-data-problems" class="section level3">
<h3><span class="header-section-number">5.0.20</span> 4.20 Correcting Data ‘Problems’</h3>
<p>Log transformation log(X<sub>i</sub>) or log(X<sub>i</sub> +1)</p>
<ul>
<li>Reduce positive skew.</li>
</ul>
<p>Square root transformation:</p>
<ul>
<li>Also reduces positive skew. Can also be useful for stabilizing variance.</li>
</ul>
<p>Reciprocal transformation (1/ X<sub>i</sub>):</p>
<ul>
<li>Dividing 1 by each score also reduces the impact of large scores.</li>
<li>This transformation reverses the scores</li>
<li>You can avoid this by reversing the scores before the transformation, 1/(X<sub>Highest</sub> - X<sub>i</sub>).</li>
</ul>
</div>
<div id="to-transform-or-not" class="section level3">
<h3><span class="header-section-number">5.0.21</span> 4.21 To Transform Or Not</h3>
<p>Transforming the data helps as often as it hinders the accuracy of F</p>
<p>The central limit theorem: sampling distribution will be normal in samples &gt; 40 anyway.</p>
<ul>
<li><p>Transforming the data changes the hypothesis being tested. + E.g. when using a log transformation and comparing means, you change from comparing arithmetic means to comparing geometric means.</p></li>
<li><p>In small samples it is tricky to determine normality one way or another. The consequences for the statistical model of applying the ‘wrong’ transformation could be worse than the consequences of analysing the untransformed scores.</p></li>
<li><p>Alternative - use non-parametric statistics or Bayesian approaches.</p></li>
</ul>
</div>
</div>
<div id="parameteric-and-non-parameteric-correlation" class="section level1">
<h1><span class="header-section-number">6</span> 5 Parameteric and Non-Parameteric Correlation</h1>
<div id="correlation" class="section level3">
<h3><span class="header-section-number">6.0.1</span> 5.1 Correlation</h3>
<p>Linear dependence of variables</p>
<ul>
<li>Scatterplots</li>
<li>Covariance</li>
<li>Pearson’s correlation coefficient</li>
</ul>
<p>Nonparametric measures</p>
<ul>
<li>Spearman’s rho</li>
<li>Kendall’s tau</li>
</ul>
<p>Interpreting correlations</p>
<ul>
<li>Causality</li>
</ul>
<p>Partial correlations</p>
</div>
<div id="what-is-a-correlation" class="section level3">
<h3><span class="header-section-number">6.0.2</span> 5.2 What is a Correlation?</h3>
<p>It is a way of measuring the extent to which two variables are related.</p>
<p>It measures the pattern of responses across variables.</p>
<p><Br></p>
<div id="small-relationship" class="section level4">
<h4><span class="header-section-number">6.0.2.1</span> Small Relationship</h4>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p><em>Figure 5.2.1</em></p>
<p><Br></p>
</div>
<div id="positive-relationship" class="section level4">
<h4><span class="header-section-number">6.0.2.2</span> Positive Relationship</h4>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p><em>Figure 5.2.2</em></p>
<p><Br></p>
</div>
<div id="negative-relationship" class="section level4">
<h4><span class="header-section-number">6.0.2.3</span> Negative Relationship</h4>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p><em>Figure 5.2.3</em></p>
<p><Br></p>
</div>
</div>
<div id="measuring-relationships" class="section level3">
<h3><span class="header-section-number">6.0.3</span> 5.3 Measuring Relationships</h3>
<p>We need to see whether as one variable increases, the other increases, decreases or stays the same.</p>
<p>This can be done by calculating the covariance.</p>
<ul>
<li>We look at how much each score deviates from the mean.</li>
<li>If both variables deviate from the mean by the same amount, they are likely to be related.</li>
</ul>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Participant</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adverts Watched</td>
<td>5</td>
<td>4</td>
<td>4</td>
<td>6</td>
<td>8</td>
<td>5.4</td>
<td>1.67</td>
</tr>
<tr class="even">
<td>Packets Bought</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>13</td>
<td>15</td>
<td>11</td>
<td>2.92</td>
</tr>
</tbody>
</table>
<p><em>Table 5.3.1:  Measuring Relationships</em></p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Participant</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>Mean</th>
<th>SD</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Adverts Watched</td>
<td>5</td>
<td>4</td>
<td>4</td>
<td>6</td>
<td>8</td>
<td>5.4</td>
<td>1.67</td>
</tr>
<tr class="even">
<td>Packets Bought</td>
<td>8</td>
<td>9</td>
<td>10</td>
<td>13</td>
<td>15</td>
<td>11</td>
<td>2.92</td>
</tr>
<tr class="odd">
<td>Advertiser Residual</td>
<td>-0.4</td>
<td>-1.4</td>
<td>-1.4</td>
<td>0.6</td>
<td>2.6</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Packets residual</td>
<td>-3</td>
<td>-2</td>
<td>-1</td>
<td>2</td>
<td>4</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><em>Table 5.3.2:  Measuring Relationships</em></p>
<p><Br></p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p><em>Figure 5.3.1</em></p>
<p><Br></p>
</div>
<div id="re-examination-of-variance" class="section level3">
<h3><span class="header-section-number">6.0.4</span> 5.4 Re-examination of Variance</h3>
<p>The variance tells us by how much scores deviate from the mean for a single variable.</p>
<p>It is closely linked to the sum of squares.</p>
<p>Covariance is similar - it tells is by how much scores on two variables differ from their respective means.</p>
<p><span class="math inline">\(variance=\frac{\Sigma(x_i - \bar{X})}{N-1}^2\)</span></p>
<p><span class="math inline">\(variance=\frac{\Sigma(x_i - \bar{X})(x_i - \bar{X})}{N-1}\)</span></p>
<p><Br></p>
</div>
<div id="covariance" class="section level3">
<h3><span class="header-section-number">6.0.5</span> 5.5 Covariance</h3>
<p>Calculate the error between the mean and each subject’s score for the first variable (x).</p>
<p>Calculate the error between the mean and their score for the second variable (y).</p>
<p>Multiply these error values.</p>
<p>Add these values and you get the cross product deviations.</p>
<p>The covariance is the average cross-product deviations:</p>
<p>Table 1000 =</p>
<p><Br></p>
<p><span class="math inline">\(N_1(a_1,_1) + N_2(a_2,_1)\)</span></p>
<p><span class="math inline">\(cov(x,y)=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{N-1}\)</span></p>
<p><span class="math inline">\(=\frac{(-0.4)(-3)+(-1.4)(-2)+(-1.4)(-1)+(0.6)(2)+(2.6)(4)}{4}\)</span></p>
<p><span class="math inline">\(=\frac{1.2+2.8+1.4+1.2+10.4}{4}\)</span></p>
<p><span class="math inline">\(=\frac{17}{4}\)</span></p>
<p><span class="math inline">\(=4.25\)</span></p>
<p><Br></p>
</div>
<div id="problems-with-covariance" class="section level3">
<h3><span class="header-section-number">6.0.6</span> 5.6 Problems with Covariance</h3>
<p>Dependent on the units of measurement.</p>
<ul>
<li>E.g. the covariance of two variables measured in miles might be 4.25, but if the same scores are converted to kilometres, the covariance is 11.</li>
</ul>
<p>One solution: standardize it</p>
<ul>
<li>Divide by the standard deviations of both variables.</li>
</ul>
<p>The standardized version of covariance is known as the correlation coefficient.</p>
<ul>
<li>It is relatively unaffected by units of measurement.</li>
</ul>
</div>
<div id="the-correlation-coefficient" class="section level3">
<h3><span class="header-section-number">6.0.7</span> 5.7 The Correlation Coefficient</h3>
<p><span class="math inline">\(r=\frac{cov_xy}{s_xs_y}\)</span></p>
<p><span class="math inline">\(=\frac{\Sigma(x_i - \bar{x})(y_i - \bar{y})}{(N-1)s_xs_y}\)</span></p>
<p><Br> <Br></p>
<p><span class="math inline">\(r=\frac{cov_xy}{s_xs_y}\)</span></p>
<p><span class="math inline">\(=\frac{4.25}{1.67 * 2.92}\)</span></p>
<p><span class="math inline">\(=0.87\)</span></p>
<p><Br></p>
<p>Termed Pearson-product moment correlation coefficient</p>
<p>It is a testable hypothesis</p>
<p><img src="Fundamentals_of_Statistics_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<p><em>Figure 5.7.1</em></p>
<p><Br></p>
<p>It is a testable hypothesis</p>
<p>Testing <span class="math inline">\(H_0: \rho=0\)</span> versus <span class="math inline">\(H_A: \rho\ne0\)</span></p>
<p>The standard error of the correlation coefficient is calculated as:</p>
<p><span class="math inline">\(S_r=\sqrt\frac{1-r^2}{n-2}\)</span></p>
<p>It is a testable hypothesis</p>
<p>r = 0.870</p>
<p>n = 12 (new data set, with more samples)</p>
<p>The critical value is:</p>
<p><Br></p>
<p><span class="math inline">\(t=\frac{r}{S_r}= \frac{0.870}{0.156}= 5.58\)</span></p>
<p>t<sub>0.05(2),10</sub> =2.228</p>
<p>Testing <span class="math inline">\(H_0: \rho=0\)</span> versus <span class="math inline">\(H_A: \rho\ne0\)</span></p>
<p><Br></p>
<p>It varies between -1 and +1</p>
<ul>
<li>0 = no relationship</li>
</ul>
<p>It is an effect size</p>
<ul>
<li><span class="math inline">\(\pm .1\)</span> = small effect</li>
<li><span class="math inline">\(\pm .3\)</span> = medium effect</li>
<li><span class="math inline">\(\pm .5\)</span> = large effect</li>
</ul>
<p>Coefficient of determination, r2</p>
<ul>
<li>By squaring the value of r you get the proportion of variance in one variable shared by the other.</li>
</ul>
</div>
<div id="correlation-and-causality" class="section level3">
<h3><span class="header-section-number">6.0.8</span> 5.8 Correlation and Causality</h3>
<p>The third-variable problem:</p>
<ul>
<li>In any correlation, causality between two variables cannot be assumed because there may be other measured or unmeasured variables affecting the results.</li>
<li>Could be many latent variables..</li>
</ul>
<p>Direction of causality:</p>
<ul>
<li>Correlation coefficients say nothing about which variable causes the other to change.</li>
</ul>
</div>
<div id="non-parametric-correlation" class="section level3">
<h3><span class="header-section-number">6.0.9</span> 5.9 Non-parametric Correlation</h3>
<p>Spearman’s rho</p>
<ul>
<li>Pearson’s correlation on the ranked data</li>
</ul>
<p>Kendall’s tau</p>
<ul>
<li>“Better” than Spearman’s for small samples</li>
</ul>
</div>
<div id="spearman-rank-correlation-coefficient" class="section level3">
<h3><span class="header-section-number">6.0.10</span> 5.10 Spearman Rank Correlation Coefficient</h3>
<p>d is the difference between two numbers in each pair of ranks</p>
<p>n = number of pairs of data</p>
<p><span class="math inline">\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)</span></p>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
</tr>
</tbody>
</table>
<p><em>Table 5.10.1:  Spearman Rank Correlation Coefficient</em></p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
<th>Rank 1</th>
<th>Rank 2</th>
<th>d</th>
<th>d<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
<td>1</td>
<td>3</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><em>Table 5.10.2:  Spearman Rank Correlation Coefficient</em></p>
<p><Br></p>
<table>
<thead>
<tr class="header">
<th>Data 1</th>
<th>Data 2</th>
<th>Rank 1</th>
<th>Rank 2</th>
<th>d</th>
<th>d<sup>2</sup></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>6</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>4</td>
<td>9</td>
<td>1</td>
<td>3</td>
<td>2</td>
<td>4</td>
</tr>
<tr class="odd">
<td>7</td>
<td>3</td>
<td>3</td>
<td>2</td>
<td>1</td>
<td>1</td>
</tr>
</tbody>
</table>
<p><em>Table 5.10.3:  Spearman Rank Correlation Coefficient</em></p>
<p><Br></p>
<p><span class="math inline">\(r=1-(\frac{6\Sigma d^2}{n(n^2 - 1)})\)</span></p>
<p><span class="math inline">\(=1-(\frac{6*6}{3(3^2 - 1)})\)</span></p>
<p><Br></p>
<p>We can use this value as the calculated r value</p>
<p>The critical value is a two tailed value with n</p>
</div>
<div id="partial-and-semi-partial-correlations" class="section level3">
<h3><span class="header-section-number">6.0.11</span> 5.11 Partial and Semi-partial Correlations</h3>
<p>Partial correlation:</p>
<ul>
<li>Measures the relationship between two variables, controlling for the effect that a third variable has on them both.</li>
</ul>
<p>Semi-partial correlation:</p>
<ul>
<li>Measures the relationship between two variables controlling for the effect that a third variable has on only one of the others.</li>
</ul>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
